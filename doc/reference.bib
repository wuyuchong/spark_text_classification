@article{2002Review,
  title={Review: Foundations of Statistical Natural Language Processing, Christopher D. Manning and Hinrich Sch√ºtze},
  author={ Manning, C. D.  and  Schtitze, H.  and  Lee, L. },
  year={2002},
 abstract={In 1993, Eugene Charniak published a slim volume entitled Statistical Language Learning. At the time, empirical techniques to natural language processing were on the rise -- that year, Computational Linguistics published a special issue on such methods -- and Charniak's text was the first to treat the emerging field.},
}

@inproceedings{Collobert2008A,
  title={A unified architecture for natural language processing: deep neural networks with multitask learning},
  author={Collobert and Ronan and Weston and Jason},
  booktitle={Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008), Helsinki, Finland, June 5-9, 2008},
  year={2008},
 abstract={We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.},
}

@article{2015From,
  title={From Visual Attributes to Adjectives through Decompositional Distributional Semantics},
  author={ Lazaridou, A.  and  Dinu, G.  and  Liska, A.  and  Baroni, M. },
  journal={Metabolism Clinical &amp; Experimental},
  volume={41},
  number={7},
  pages={800-4},
  year={2015},
 abstract={As automated image analysis progresses, there is increasing interest in richer linguistic annotation of pictures, with attributes of objects (e.g., furry, brown...) attracting most attention. By building on the recent "zero-shot learning" approach, and paying attention to the linguistic nature of attributes as noun modifiers, and specifically adjectives, we show that it is possible to tag images with attribute-denoting adjectives even when no training data containing the relevant annotation are available. Our approach relies on two key observations. First, objects can be seen as bundles of attributes, typically expressed as adjectival modifiers (a dog is something furry, brown, etc.), and thus a function trained to map visual representations of objects to nominal labels can implicitly learn to map attributes to adjectives. Second, objects and attributes come together in pictures (the same thing is a dog and it is brown). We can thus achieve better attribute (and object) label retrieval by treating images as "visual phrases", and decomposing their linguistic representation into an attribute-denoting adjective and an object-denoting noun. Our approach performs comparably to a method exploiting manual attribute annotation, it outperforms various competitive alternatives in both attribute and object annotation, and it automatically constructs attribute-centric representations that significantly improve performance in supervised object recognition.},
}

@article{WUYUNTANA2017Distributed,
  title={Distributed Representations of Mongolian Words and Its Efficient Estimation},
  author={WUYUNTANA and  Wang, S. G. },
  year={2017},
 abstract={The word vectors has good semantic properties that can be used to improve and simplify many natural language processing applications. In this paper, we use the two model architectures Continuous Bag-of-Words (CBOW) and skip-gram to compute the Mongolian word vectors. On this basis, we design a comprehensive test set based on the Mongolia language features to measure the similarity of Mongolian syntactic and semantic. And then on this test set estimate the quality of the Mongolian word vectors. Experiments show that the Skip-gram architecture works better than the CBOW on the Mongolian syntactic semantic tasks, and the word vectors computed by this model are have good quality. Take Mongolian verb vectors as an example, also find that there are multiple similarities between the computed Mongolian word vectors.},
}

@article{0An,
  title={An updated set of basic linear algebra subprograms (BLAS)},
  author={ Article, G. T. },
 abstract={An abstract is not available.},
}

@inproceedings{2016Multi,
  title={Multi-Person Tracking by Multicut and Deep Matching},
  author={ Tang, S.  and  Andres, B.  and  Andriluka, M.  and  Schiele, B. },
  booktitle={Springer International Publishing},
  year={2016},
 abstract={In [1], we proposed a graph-based formulation that links and clusters person hypotheses over time by solving a minimum cost subgraph multicut problem. In this paper, we modify and extend [1] in three ways: 1) We introduce a novel local pairwise feature based on local appearance matching that is robust to partial occlusion and camera motion. 2) We perform extensive experiments to compare different pairwise potentials and to analyze the robustness of the tracking formulation. 3) We consider a plain multicut problem and remove outlying clusters from its solution. This allows us to employ an efficient primal feasible optimization algorithm that is not applicable to the subgraph multicut problem of [1]. Unlike the branch-and-cut algorithm used there, this efficient algorithm used here is applicable to long videos and many detections. Together with the novel feature, it eliminates the need for the intermediate tracklet representation of [1]. We demonstrate the effectiveness of our overall approach on the MOT16 benchmark [2], achieving state-of-art performance.},
}

@article{2016BlackOut,
  title={BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies},
  author={ Ji, S.  and  Vishwanathan, Svn  and  Satish, N.  and  Anderson, M. J.  and  Dubey, P. },
  journal={Computer ence},
  volume={115},
  number={8},
  pages={2159-68},
  year={2016},
 abstract={We propose BlackOut, an approximation algorithm to efficiently train massive recurrent neural network language models (RNNLMs) with million word vocabularies. BlackOut is motivated by using a discriminative loss, and we describe a new sampling strategy which significantly reduces computation while improving stability, sample efficiency, and rate of convergence. One way to understand BlackOut is to view it as an extension of the DropOut strategy to the output layer, wherein we use a discriminative training loss and a weighted sampling scheme. We also establish close connections between BlackOut, importance sampling, and noise contrastive estimation (NCE). Our experiments, on the recently released one billion word language modeling benchmark, demonstrate scalability and accuracy of BlackOut; we outperform the state-of-the art, and achieve the lowest perplexity scores on this dataset. Moreover, unlike other established methods which typically require GPUs or CPU clusters, we show that a carefully implemented version of BlackOut requires only 1-10 days on a single machine to train a RNNLM with a million word vocabulary and billions of parameters on one billion words. Although we describe BlackOut in the context of RNNLM training, it can be used to any networks with large softmax output layers.},
}

